{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Autoencoders (dA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Denoising Autoencoder (dA)는 전통적인 autoencoder이고 deep network을 대한 block을 쌓기 위해서 도입됨.\n",
    "- Autoencoders에 대해서 먼저 짧게 배워보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoders\n",
    "- autoencoder는 입력값 \\\\( x \\in {[0,1]}^{d} \\\\)과 첫번째로 미리 결정된 mapping을 통해서  hidden 표현식 \\\\( y \\in [0,1]^{d'} \\\\) 로 맵핑함.\n",
    "- \\\\( y = s( {W}{x} + {b} ) \\\\)\n",
    "- s는 sigmoid와 같이 비선형임. \n",
    "\n",
    "\n",
    "- 이후에 보여지는 표현 \\\\( {y} \\\\) 또는 code는 reconstruction \\\\( {z} \\\\)에 의해서 \\\\( {x} \\\\) 와 같은 모양으로  역맵핑함.\n",
    "- 맵핑은 비슷한 변환 방식으로 이루어짐.\n",
    "- \\\\( {z} = s({W'}{y} + {b'} ) \\\\)\n",
    "\n",
    "\n",
    "\n",
    "- (prime symbol은 전치행렬을 의미하지는 않음.) \\\\( {z} \\\\) 는 주어진 code \\\\( {y} \\\\)으로 \\\\( {x} \\\\)을 추정한 것으로 보여짐.\n",
    "- 역맵핑에 대한 가중치 행렬 \\\\( {W'} \\\\)는 forward 맵핑에 대한 전치를 포함함 : \\\\( {W'} = {W}^T \\\\) .\n",
    "- 이 모델의 파라메타( \\\\( {W}, {b}, {b'}, {W'}  \\\\) )는 평균 reconstruction error을 최소화하는 방식으로 최적화되어짐.\n",
    "\n",
    "\n",
    "- reconstruction error는 주어진 code에 대한 입력값의 적합한 분포 가정을 기반으로 여러가지 방식으로 측정할 수 있음.\n",
    "- 전통적인 squared error \\\\( L({x} {z}) = || {x} - {z} ||^2 \\\\)을 사용할 수 있음.\n",
    "- 입력값이 0, 1 또는 0 ,1 에 대한 확률일때는 reconstruction의  cross-entropy값을 사용할 수 있음.\n",
    "![이미지](http://www.deeplearning.net/tutorial/_images/math/499a4658ab7ecc5de5c19c7cc89a53205f3ecdf9.png)\n",
    " \n",
    " \n",
    "- code \\\\( {y} \\\\)는 데이터에서 변동의 주요소에 의해서 설명될 수 있음.\n",
    "- 이것은 주성분분석( PCA )와 비슷함.\n",
    "- 특히 하나의 hidden layer가 있고 mean squared error로 네트워크를 학습하였으면, k hidden unit은 k개의 주성분분석과 비슷함.\n",
    "- hidden layer가 비선형이라면, auto-encoder PCA와 다르게 됨.\n",
    "\n",
    "\n",
    "- \\\\( {y} \\\\)는  \\\\( {x} \\\\)의 손실압축과 비슷하게 보임.\n",
    "\n",
    "\n",
    "- class형식으로 Theano을 가지고 auto-encoder을 구현해보자. \n",
    "- 첫번째는 autoencoder의 파라메타 ( \\\\( {W}, {b}, {b'}  \\\\) )을 shared 변수를 만듬\n",
    "- \\\\( {W}^T \\\\)는 \\\\({W'}\\\\)을 위해서 사용됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def __init__(\n",
    "        self,\n",
    "        numpy_rng,\n",
    "        theano_rng=None,\n",
    "        input=None,\n",
    "        n_visible=784,\n",
    "        n_hidden=500,\n",
    "        W=None,\n",
    "        bhid=None,\n",
    "        bvis=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        dA 클래스의 생성자\n",
    "        visible units의 수(입력값의 차원),  hidden units의 수, corruption level을 설정, \n",
    "        입력으로 symbolic 변수를 받음.\n",
    "        SdAs에서는 첫번째 dA의 결과가 두번째 dA의 입력으로 갖음.\n",
    "\n",
    "        :type numpy_rng: numpy.random.RandomState\n",
    "        :param numpy_rng: number random generator used to generate weights\n",
    "\n",
    "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
    "        :param theano_rng: Theano random generator; if None is given one is\n",
    "                     generated based on a seed drawn from `rng`\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: a symbolic description of the input or None for\n",
    "                      standalone dA\n",
    "\n",
    "        :type n_visible: int\n",
    "        :param n_visible: number of visible units\n",
    "\n",
    "        :type n_hidden: int\n",
    "        :param n_hidden:  number of hidden units\n",
    "\n",
    "        :type W: theano.tensor.TensorType\n",
    "        :param W: Theano variable pointing to a set of weights that should be\n",
    "                  shared belong the dA and another architecture; if dA should\n",
    "                  be standalone set this to None\n",
    "\n",
    "        :type bhid: theano.tensor.TensorType\n",
    "        :param bhid: Theano variable pointing to a set of biases values (for\n",
    "                     hidden units) that should be shared belong dA and another\n",
    "                     architecture; if dA should be standalone set this to None\n",
    "\n",
    "        :type bvis: theano.tensor.TensorType\n",
    "        :param bvis: Theano variable pointing to a set of biases values (for\n",
    "                     visible units) that should be shared belong dA and another\n",
    "                     architecture; if dA should be standalone set this to None\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        # create a Theano random generator that gives symbolic random values\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        # note : W' was written as `W_prime` and b' as `b_prime`\n",
    "        if not W:\n",
    "            # W is initialized with `initial_W` which is uniformely sampled\n",
    "            # from -4*sqrt(6./(n_visible+n_hidden)) and\n",
    "            # 4*sqrt(6./(n_hidden+n_visible))the output of uniform if\n",
    "            # converted using asarray to dtype\n",
    "            # theano.config.floatX so that the code is runable on GPU\n",
    "            initial_W = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible, n_hidden)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "        if not bvis:\n",
    "            bvis = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_visible,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        if not bhid:\n",
    "            bhid = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='b',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        self.W = W\n",
    "        # b corresponds to the bias of the hidden\n",
    "        self.b = bhid\n",
    "        # b_prime corresponds to the bias of the visible\n",
    "        self.b_prime = bvis\n",
    "        # tied weights, therefore W_prime is W transpose\n",
    "        self.W_prime = self.W.T\n",
    "        self.theano_rng = theano_rng\n",
    "        # if no input is given, generate a variable representing the input\n",
    "        if input is None:\n",
    "            # we use a matrix because we expect a minibatch of several\n",
    "            # examples, each example being a row\n",
    "            self.x = T.dmatrix(name='input')\n",
    "        else:\n",
    "            self.x = input\n",
    "\n",
    "        self.params = [self.W, self.b, self.b_prime]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the symbolic output (the \\\\( {y} \\\\) above) of layer k will be the symbolic input of layer k+1.\n",
    "\n",
    "- hidden layer에서의 수식과 복원식을 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def get_hidden_values(self, input):\n",
    "        \"\"\" Computes the values of the hidden layer \"\"\"\n",
    "        return T.nnet.sigmoid(T.dot(input, self.W) + self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def get_reconstructed_input(self, hidden):\n",
    "        \"\"\"Computes the reconstructed input given the values of the\n",
    "        hidden layer\n",
    "        \"\"\"\n",
    "        return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cost을 계산하고 stochastic gradient descent 통해서 파라메타를 업데이트함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def get_cost_updates(self, corruption_level, learning_rate):\n",
    "        \"\"\" This function computes the cost and the updates for one trainng\n",
    "        step of the dA \"\"\"\n",
    "\n",
    "        tilde_x = self.get_corrupted_input(self.x, corruption_level)\n",
    "        y = self.get_hidden_values(tilde_x)\n",
    "        z = self.get_reconstructed_input(y)\n",
    "        # note : we sum over the size of a datapoint; if we are using\n",
    "        #        minibatches, L will be a vector, with one entry per\n",
    "        #        example in minibatch\n",
    "        L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1)\n",
    "        # note : L is now a vector, where each element is the\n",
    "        #        cross-entropy cost of the reconstruction of the\n",
    "        #        corresponding example of the minibatch. We need to\n",
    "        #        compute the average of all these to get the cost of\n",
    "        #        the minibatch\n",
    "        cost = T.mean(L)\n",
    "\n",
    "        # compute the gradients of the cost of the `dA` with respect\n",
    "        # to its parameters\n",
    "        gparams = T.grad(cost, self.params)\n",
    "        # generate the list of updates\n",
    "        updates = [\n",
    "            (param, param - learning_rate * gparam)\n",
    "            for param, gparam in zip(self.params, gparams)\n",
    "        ]\n",
    "\n",
    "        return (cost, updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 복원 cost을 최소화하는 W, b, b_primer와 같은 parameter을 업데이터하는 반복적인 함수를 정의함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    da = dA(\n",
    "        numpy_rng=rng,\n",
    "        theano_rng=theano_rng,\n",
    "        input=x,\n",
    "        n_visible=28 * 28,\n",
    "        n_hidden=500\n",
    "    )\n",
    "\n",
    "    cost, updates = da.get_cost_updates(\n",
    "        corruption_level=0.,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    train_da = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    ############\n",
    "    # TRAINING #\n",
    "    ############\n",
    "\n",
    "    # go through training epochs\n",
    "    for epoch in xrange(training_epochs):\n",
    "        # go through trainng set\n",
    "        c = []\n",
    "        for batch_index in xrange(n_train_batches):\n",
    "            c.append(train_da(batch_index))\n",
    "\n",
    "        print 'Training epoch %d, cost ' % epoch, numpy.mean(c)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    training_time = (end_time - start_time)\n",
    "\n",
    "    print >> sys.stderr, ('The no corruption code for file ' +\n",
    "                          os.path.split(__file__)[1] +\n",
    "                          ' ran for %.2fm' % ((training_time) / 60.))\n",
    "    image = Image.fromarray(\n",
    "        tile_raster_images(X=da.W.get_value(borrow=True).T,\n",
    "                           img_shape=(28, 28), tile_shape=(10, 10),\n",
    "                           tile_spacing=(1, 1)))\n",
    "    image.save('filters_corruption_0.png')\n",
    "\n",
    "    # start-snippet-3\n",
    "    #####################################\n",
    "    # BUILDING THE MODEL CORRUPTION 30% #\n",
    "    #####################################\n",
    "\n",
    "    rng = numpy.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    da = dA(\n",
    "        numpy_rng=rng,\n",
    "        theano_rng=theano_rng,\n",
    "        input=x,\n",
    "        n_visible=28 * 28,\n",
    "        n_hidden=500\n",
    "    )\n",
    "\n",
    "    cost, updates = da.get_cost_updates(\n",
    "        corruption_level=0.3,\n",
    "        learning_rate=learning_rate\n",
    "    )\n",
    "\n",
    "    train_da = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    ############\n",
    "    # TRAINING #\n",
    "    ############\n",
    "\n",
    "    # go through training epochs\n",
    "    for epoch in xrange(training_epochs):\n",
    "        # go through trainng set\n",
    "        c = []\n",
    "        for batch_index in xrange(n_train_batches):\n",
    "            c.append(train_da(batch_index))\n",
    "\n",
    "        print 'Training epoch %d, cost ' % epoch, numpy.mean(c)\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    training_time = (end_time - start_time)\n",
    "\n",
    "    print >> sys.stderr, ('The 30% corruption code for file ' +\n",
    "                          os.path.split(__file__)[1] +\n",
    "                          ' ran for %.2fm' % (training_time / 60.))\n",
    "    # end-snippet-3\n",
    "\n",
    "    # start-snippet-4\n",
    "    image = Image.fromarray(tile_raster_images(\n",
    "        X=da.W.get_value(borrow=True).T,\n",
    "        img_shape=(28, 28), tile_shape=(10, 10),\n",
    "        tile_spacing=(1, 1)))\n",
    "    image.save('filters_corruption_30.png')\n",
    "    # end-snippet-4\n",
    "\n",
    "    os.chdir('../')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_dA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력보다 많은 hidden unit을 갖는 auto-encoder는 유용한 identity function을 학습에 방해가 됨.\n",
    "- hidden unit이 zero 또는 near-zero값을 갖는 것을 희소성이라고 함.\n",
    "- 희소성은 많은 경우에 매우 성공적으로 발견됨.\n",
    "- 입력으로부터 복원되는 변환에서 randomness( 임의성 )이 추가되어지고, RBM이나 Denoising Auto-Encoders에서 사용되는 기술임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- denoising autoencoders의 아이디어는 단순함.\n",
    "- 강건한 특징을 발견하고 단순한 identity 학습을 방해받지 않도록 hidden layer을 강화하기 위해서, corrupted(오염된) version으로부터 입력을 복원하는 autoencoder을 학습함.\n",
    "\n",
    "\n",
    "- denoising auto-encoder는 auto-encoder의 stochastic(확률) version임.\n",
    "- 직관적으로 denoising auto-encoder는 2가지 일을 함.\n",
    "   - 1) 입력값을 encode 하고 auto-encoder의 입력값이 적용되어 확률적으로 불순도 과정의 효과를 제거함. \n",
    "   - 2) 입력 사이에서 통계적인 의존성을 잡아줌.\n",
    "- denoising auto-encoder는 여러 가지 관점( 매니 폴드 학습 관점, 확률 운영자의 관점, 상향식 (bottom-up) - 정보 이론 관점, 탑 - 다운 - 생식 모델의 관점  )으로 이해할 수 있음.\n",
    "\n",
    "\n",
    "- 확률적인 불순도 과정은 입력의 어떤 부분은 zero로 설정함.\n",
    "- 그러므로, denoising auto-encoder은 오염되지 않은 (i.e., non-missing) 값으로부터 오염된 값을 예측할려고 함.\n",
    "- 나머지로부터 변수들의 서브셋을 예측하는 것은 변수의 집합들로부터 연결분포( joint distribut)을 완벽히 잡아낸으로 충분한 조건인 것을 주목하자.   Note how being able to predict any subset of variables from the rest is a sufficient condition for completely capturing the joint distribution between a set of variables (this is how Gibbs sampling works).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- autoencoder 클래스를   denoising autoencoder 클래스로 변경하기 위해서, 입력값을 확률적인 오염시키는 과정이 필요함.\n",
    "- 입력값은 여러 가지 방법으로 오염시킬 수 있으며, 여기에서는 램덤하게 값을 zero을 만들어주는 오염과정을 거침."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def get_corrupted_input(self, input, corruption_level):\n",
    "        \"\"\"This function keeps ``1-corruption_level`` entries of the inputs the\n",
    "        same and zero-out randomly selected subset of size ``coruption_level``\n",
    "        Note : first argument of theano.rng.binomial is the shape(size) of\n",
    "               random numbers that it should produce\n",
    "               second argument is the number of trials\n",
    "               third argument is the probability of success of any trial\n",
    "\n",
    "                this will produce an array of 0s and 1s where 1 has a\n",
    "                probability of 1 - ``corruption_level`` and 0 with\n",
    "                ``corruption_level``\n",
    "\n",
    "                The binomial function return int64 data type by\n",
    "                default.  int64 multiplicated by the input\n",
    "                type(floatX) always return float64.  To keep all data\n",
    "                in floatX when floatX is float32, we set the dtype of\n",
    "                the binomial to floatX. As in our case the value of\n",
    "                the binomial is always 0 or 1, this don't change the\n",
    "                result. This is needed to allow the gpu to work\n",
    "                correctly as it only support float32 for now.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
    "                                        p=1 - corruption_level,\n",
    "                                        dtype=theano.config.floatX) * input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stacked autoencoder class (Stacked Autoencoders)에서는 dA의 가중치가 sigmoid layer에 대응되는 것과 값이 공유됨.\n",
    "- 그 이유는 dA의 생성자는 shared parameters로 Theano variables을 갖음.\n",
    "- 만약, 다른  parameter들은 None으로 처리함. \n",
    "- 새로운 생성자를 만들어 보자.\n",
    "\n",
    "\n",
    "- 아래는 최종 denoising autoencoder class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dA(object):\n",
    "    \"\"\"Denoising Auto-Encoder class (dA)\n",
    "\n",
    "    A denoising autoencoders tries to reconstruct the input from a corrupted\n",
    "    version of it by projecting it first in a latent space and reprojecting\n",
    "    it afterwards back in the input space. Please refer to Vincent et al.,2008\n",
    "    for more details. If x is the input then equation (1) computes a partially\n",
    "    destroyed version of x by means of a stochastic mapping q_D. Equation (2)\n",
    "    computes the projection of the input into the latent space. Equation (3)\n",
    "    computes the reconstruction of the input, while equation (4) computes the\n",
    "    reconstruction error.\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        \\tilde{x} ~ q_D(\\tilde{x}|x)                                     (1)\n",
    "\n",
    "        y = s(W \\tilde{x} + b)                                           (2)\n",
    "\n",
    "        x = s(W' y  + b')                                                (3)\n",
    "\n",
    "        L(x,z) = -sum_{k=1}^d [x_k \\log z_k + (1-x_k) \\log( 1-z_k)]      (4)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        numpy_rng,\n",
    "        theano_rng=None,\n",
    "        input=None,\n",
    "        n_visible=784,\n",
    "        n_hidden=500,\n",
    "        W=None,\n",
    "        bhid=None,\n",
    "        bvis=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dA class by specifying the number of visible units (the\n",
    "        dimension d of the input ), the number of hidden units ( the dimension\n",
    "        d' of the latent or hidden space ) and the corruption level. The\n",
    "        constructor also receives symbolic variables for the input, weights and\n",
    "        bias. Such a symbolic variables are useful when, for example the input\n",
    "        is the result of some computations, or when weights are shared between\n",
    "        the dA and an MLP layer. When dealing with SdAs this always happens,\n",
    "        the dA on layer 2 gets as input the output of the dA on layer 1,\n",
    "        and the weights of the dA are used in the second stage of training\n",
    "        to construct an MLP.\n",
    "\n",
    "        :type numpy_rng: numpy.random.RandomState\n",
    "        :param numpy_rng: number random generator used to generate weights\n",
    "\n",
    "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
    "        :param theano_rng: Theano random generator; if None is given one is\n",
    "                     generated based on a seed drawn from `rng`\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: a symbolic description of the input or None for\n",
    "                      standalone dA\n",
    "\n",
    "        :type n_visible: int\n",
    "        :param n_visible: number of visible units\n",
    "\n",
    "        :type n_hidden: int\n",
    "        :param n_hidden:  number of hidden units\n",
    "\n",
    "        :type W: theano.tensor.TensorType\n",
    "        :param W: Theano variable pointing to a set of weights that should be\n",
    "                  shared belong the dA and another architecture; if dA should\n",
    "                  be standalone set this to None\n",
    "\n",
    "        :type bhid: theano.tensor.TensorType\n",
    "        :param bhid: Theano variable pointing to a set of biases values (for\n",
    "                     hidden units) that should be shared belong dA and another\n",
    "                     architecture; if dA should be standalone set this to None\n",
    "\n",
    "        :type bvis: theano.tensor.TensorType\n",
    "        :param bvis: Theano variable pointing to a set of biases values (for\n",
    "                     visible units) that should be shared belong dA and another\n",
    "                     architecture; if dA should be standalone set this to None\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.n_visible = n_visible\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        # create a Theano random generator that gives symbolic random values\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        # note : W' was written as `W_prime` and b' as `b_prime`\n",
    "        if not W:\n",
    "            # W is initialized with `initial_W` which is uniformely sampled\n",
    "            # from -4*sqrt(6./(n_visible+n_hidden)) and\n",
    "            # 4*sqrt(6./(n_hidden+n_visible))the output of uniform if\n",
    "            # converted using asarray to dtype\n",
    "            # theano.config.floatX so that the code is runable on GPU\n",
    "            initial_W = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
    "                    size=(n_visible, n_hidden)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "        if not bvis:\n",
    "            bvis = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_visible,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        if not bhid:\n",
    "            bhid = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='b',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "        self.W = W\n",
    "        # b corresponds to the bias of the hidden\n",
    "        self.b = bhid\n",
    "        # b_prime corresponds to the bias of the visible\n",
    "        self.b_prime = bvis\n",
    "        # tied weights, therefore W_prime is W transpose\n",
    "        self.W_prime = self.W.T\n",
    "        self.theano_rng = theano_rng\n",
    "        # if no input is given, generate a variable representing the input\n",
    "        if input is None:\n",
    "            # we use a matrix because we expect a minibatch of several\n",
    "            # examples, each example being a row\n",
    "            self.x = T.dmatrix(name='input')\n",
    "        else:\n",
    "            self.x = input\n",
    "\n",
    "        self.params = [self.W, self.b, self.b_prime]\n",
    "\n",
    "    def get_corrupted_input(self, input, corruption_level):\n",
    "        \"\"\"This function keeps ``1-corruption_level`` entries of the inputs the\n",
    "        same and zero-out randomly selected subset of size ``coruption_level``\n",
    "        Note : first argument of theano.rng.binomial is the shape(size) of\n",
    "               random numbers that it should produce\n",
    "               second argument is the number of trials\n",
    "               third argument is the probability of success of any trial\n",
    "\n",
    "                this will produce an array of 0s and 1s where 1 has a\n",
    "                probability of 1 - ``corruption_level`` and 0 with\n",
    "                ``corruption_level``\n",
    "\n",
    "                The binomial function return int64 data type by\n",
    "                default.  int64 multiplicated by the input\n",
    "                type(floatX) always return float64.  To keep all data\n",
    "                in floatX when floatX is float32, we set the dtype of\n",
    "                the binomial to floatX. As in our case the value of\n",
    "                the binomial is always 0 or 1, this don't change the\n",
    "                result. This is needed to allow the gpu to work\n",
    "                correctly as it only support float32 for now.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
    "                                        p=1 - corruption_level,\n",
    "                                        dtype=theano.config.floatX) * input\n",
    "\n",
    "    def get_hidden_values(self, input):\n",
    "        \"\"\" Computes the values of the hidden layer \"\"\"\n",
    "        return T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n",
    "\n",
    "    def get_reconstructed_input(self, hidden):\n",
    "        \"\"\"Computes the reconstructed input given the values of the\n",
    "        hidden layer\n",
    "\n",
    "        \"\"\"\n",
    "        return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)\n",
    "\n",
    "    def get_cost_updates(self, corruption_level, learning_rate):\n",
    "        \"\"\" This function computes the cost and the updates for one trainng\n",
    "        step of the dA \"\"\"\n",
    "\n",
    "        tilde_x = self.get_corrupted_input(self.x, corruption_level)\n",
    "        y = self.get_hidden_values(tilde_x)\n",
    "        z = self.get_reconstructed_input(y)\n",
    "        # note : we sum over the size of a datapoint; if we are using\n",
    "        #        minibatches, L will be a vector, with one entry per\n",
    "        #        example in minibatch\n",
    "        L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1)\n",
    "        # note : L is now a vector, where each element is the\n",
    "        #        cross-entropy cost of the reconstruction of the\n",
    "        #        corresponding example of the minibatch. We need to\n",
    "        #        compute the average of all these to get the cost of\n",
    "        #        the minibatch\n",
    "        cost = T.mean(L)\n",
    "\n",
    "        # compute the gradients of the cost of the `dA` with respect\n",
    "        # to its parameters\n",
    "        gparams = T.grad(cost, self.params)\n",
    "        # generate the list of updates\n",
    "        updates = [\n",
    "            (param, param - learning_rate * gparam)\n",
    "            for param, gparam in zip(self.params, gparams)\n",
    "        ]\n",
    "\n",
    "        return (cost, updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 예제 코드 : http://deeplearning.net/tutorial/code/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-02-02 16:37:49--  http://deeplearning.net/tutorial/code/dA.py\n",
      "Resolving deeplearning.net (deeplearning.net)... 132.204.26.28\n",
      "Connecting to deeplearning.net (deeplearning.net)|132.204.26.28|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 14736 (14K) [text/plain]\n",
      "Saving to: ‘dA.py’\n",
      "\n",
      "100%[======================================>] 14,736      --.-K/s   in 0s      \n",
      "\n",
      "2016-02-02 16:37:50 (337 MB/s) - ‘dA.py’ saved [14736/14736]\n",
      "\n",
      "--2016-02-02 16:37:50--  http://deeplearning.net/tutorial/code/utils.py\n",
      "Resolving deeplearning.net (deeplearning.net)... 132.204.26.28\n",
      "Connecting to deeplearning.net (deeplearning.net)|132.204.26.28|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5101 (5.0K) [text/plain]\n",
      "Saving to: ‘utils.py’\n",
      "\n",
      "100%[======================================>] 5,101       --.-K/s   in 0s      \n",
      "\n",
      "2016-02-02 16:37:50 (437 MB/s) - ‘utils.py’ saved [5101/5101]\n",
      "\n",
      "--2016-02-02 16:37:50--  http://deeplearning.net/tutorial/code/logistic_sgd.py\n",
      "Resolving deeplearning.net (deeplearning.net)... 132.204.26.28\n",
      "Connecting to deeplearning.net (deeplearning.net)|132.204.26.28|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 17006 (17K) [text/plain]\n",
      "Saving to: ‘logistic_sgd.py’\n",
      "\n",
      "100%[======================================>] 17,006      73.1KB/s   in 0.2s   \n",
      "\n",
      "2016-02-02 16:37:51 (73.1 KB/s) - ‘logistic_sgd.py’ saved [17006/17006]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! rm -f dA.py utils.py logistic_sgd.py\n",
    "\n",
    "! wget http://deeplearning.net/tutorial/code/dA.py\n",
    "! wget http://deeplearning.net/tutorial/code/utils.py\n",
    "! wget http://deeplearning.net/tutorial/code/logistic_sgd.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 deepbio deepbio 14736 Feb  2 16:05 dA.py\r\n",
      "-rw-rw-r-- 1 deepbio deepbio 17006 Feb  2 16:05 logistic_sgd.py\r\n",
      "-rw-rw-r-- 1 deepbio deepbio  5101 Feb  2 16:05 utils.py\r\n"
     ]
    }
   ],
   "source": [
    "! ls -al *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "Training epoch 0, cost  63.2891694201\n",
      "Training epoch 1, cost  55.7866565443\n",
      "Training epoch 2, cost  54.7631168984\n",
      "Training epoch 3, cost  54.2420533514\n",
      "Training epoch 4, cost  53.888670659\n",
      "Training epoch 5, cost  53.6203505434\n",
      "Training epoch 6, cost  53.4037459012\n",
      "Training epoch 7, cost  53.2219976788\n",
      "Training epoch 8, cost  53.0658010178\n",
      "Training epoch 9, cost  52.9295596873\n",
      "Training epoch 10, cost  52.8094163525\n",
      "Training epoch 11, cost  52.7024367362\n",
      "Training epoch 12, cost  52.606310148\n",
      "Training epoch 13, cost  52.5191693641\n",
      "Training epoch 14, cost  52.4395240004\n",
      "The no corruption code for file dA.py ran for 8.85m\n",
      "Training epoch 0, cost  81.7714190632\n",
      "Training epoch 1, cost  73.4285756365\n",
      "Training epoch 2, cost  70.8632686268\n",
      "Training epoch 3, cost  69.3396642015\n",
      "Training epoch 4, cost  68.4134660704\n",
      "Training epoch 5, cost  67.723705304\n",
      "Training epoch 6, cost  67.2401360252\n",
      "Training epoch 7, cost  66.849303071\n",
      "Training epoch 8, cost  66.5663948395\n",
      "Training epoch 9, cost  66.3591257941\n",
      "Training epoch 10, cost  66.1336658308\n",
      "Training epoch 11, cost  65.9893924612\n",
      "Training epoch 12, cost  65.8344131768\n",
      "Training epoch 13, cost  65.7185348901\n",
      "Training epoch 14, cost  65.6010749532\n",
      "The 30% corruption code for file dA.py ran for 8.90m\n"
     ]
    }
   ],
   "source": [
    "! python dA.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![이미지](dA_plots/filters_corruption_0.png) \n",
    "![이미지](dA_plots/filters_corruption_30.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
